{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Learning Digit Recogonizer\n",
    "\n",
    "This project shows how to make a number recognizer using a neural network that was built from scratch with numpy. The interactive learning module is one of the best parts of this project. It lets users draw their own numbers and check the model's estimates. If the model's guess is wrong, it uses what the user types as an example to get better at recogonizing the digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Libraries Used\n",
    "1. Numpy\n",
    "2. Matplotlib\n",
    "3. OpenCV\n",
    "4. Scitkit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Neural Network\n",
    "\n",
    "<center><img src=\"Neural_Net.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Dataset Used\n",
    "\n",
    "https://www.kaggle.com/datasets/animatronbot/mnist-digit-recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## References\n",
    "\n",
    "The neural network in this project was inspired by [Samson Zhang's video](https://www.youtube.com/watch?v=w8yWXqWQYmU&pp=ygUbbmV1cmFsIG5ldHdvcmsgZnJvbSBzY3JhdGNo), where he beautifully explains the concepts and demonstrates the code. While the dataset remains the same, this repository presents my own interpretation and implementation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Code\n",
    "\n",
    "Below is the necessary math and code to implement the math for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import load_data\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training input: (37800, 784)\n",
      "Shape of training output: (37800,)\n",
      "Shape of testing input: (4200, 784)\n",
      "Shape of testing output: (4200,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of training input: {X_train.shape}')\n",
    "print(f'Shape of training output: {Y_train.shape}')\n",
    "print(f'Shape of testing input: {X_test.shape}')\n",
    "print(f'Shape of testing output: {Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining activation functions of the layers\n",
    "\n",
    "$\n",
    "    \\sigma = \\text{ReLU function} \\\\\n",
    "    g = \\text{Softmax function}\n",
    "$\n",
    "$$\\begin{align}\n",
    "    \\sigma(Z) &= \n",
    "    \\begin{cases} \n",
    "        Z & \\text{if } Z > 0 \\\\\n",
    "        0 & \\text{if } Z \\leq 0 \n",
    "    \\end{cases}\\\\\n",
    "    g(Z_i) &= \\dfrac{e^{Z_i}}{\\sum e^{Z_j}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining activation functions\n",
    "def ReLU(z):  # sigma\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z):  # g\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot function\n",
    "\n",
    "The function `label_conversion()` converts the labels given in the dataset into vectors corresponding to the softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting label into softmax labels\n",
    "def label_conversion(y):\n",
    "    y_converted = np.zeros((y.size, y.max() + 1))\n",
    "    y_converted[np.arange(y.size), y] = 1\n",
    "    return y_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the parameters\n",
    "$\n",
    "    \\text{Hidden Layer 1:}\n",
    "$\n",
    "    $$ \\begin{align}\n",
    "        \\text{Dimension of }W^{[1]} &= (784 \\times 128) \\\\\n",
    "        \\text{Dimension of }b^{[1]} &= (1 \\times 128) \\\\\n",
    "    \\end{align}$$\n",
    "$\n",
    "    \\text{Hidden Layer 2:}\n",
    "$\n",
    "    $$\\begin{align}\n",
    "        \\text{Dimension of }W^{[2]} &= (128 \\times 64) \\\\\n",
    "        \\text{Dimension of }b^{[2]} &= (1 \\times 64) \\\\\n",
    "    \\end{align}$$\n",
    "$\n",
    "    \\text{Output Layer:}\n",
    "$\n",
    "    $$\\begin{align}\n",
    "        \\text{Dimension of }W^{[3]} &= (64 \\times 10) \\\\\n",
    "        \\text{Dimension of }b^{[3]} &= (1 \\times 10) \\\\\n",
    "    \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to initialize parameters to initialize parameters\n",
    "def init_params():\n",
    "    W1 = np.random.randn(784, 128) * np.sqrt(2. / 784)\n",
    "    b1 = np.zeros((1, 128))\n",
    "    W2 = np.random.randn(128, 64) * np.sqrt(2. / 128)\n",
    "    b2 = np.zeros((1, 64))\n",
    "    W3 = np.random.randn(64, 10) * np.sqrt(2. / 64)\n",
    "    b3 = np.zeros((1, 10))\n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate loss\n",
    "\n",
    "Loss function uses cross entropy loss function\n",
    "\n",
    "$$\n",
    "    \\text{Loss} = \\sum y_{i} log(A^{[3]}_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to calculate the cost\n",
    "def calculate_cost(A3, Y):\n",
    "    m = Y.shape[0]\n",
    "    Y_one_hot = label_conversion(Y)\n",
    "    epsilon = 1e-15\n",
    "    A3 = np.clip(A3, epsilon, 1 - epsilon)\n",
    "    log_probs = -np.sum(Y_one_hot * np.log(A3)) / m\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get the prediction and the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(A3):\n",
    "    return np.argmax(A3, axis=1)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.mean(predictions == Y) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining forward propagation\n",
    "$\n",
    "    \\text{Hidden Layer 1:} \\\\\n",
    "$\n",
    "    $$\\begin{align}\n",
    "        Z^{[1]} &= X \\cdot W^{[1]} + b^{[1]} \\\\\n",
    "        A^{[1]} &= \\sigma(Z^{[1]}) \\\\\n",
    "    \\end{align}$$\n",
    "$\n",
    "    \\text{Hidden Layer 2:} \\\\\n",
    "$\n",
    "    $$\\begin{align}\n",
    "        Z^{[2]} &= A^{[1]} \\cdot W^{[2]} + b^{[2]} \\\\\n",
    "        A^{[2]} &= \\sigma(Z^{[2]}) \\\\\n",
    "    \\end{align}$$\n",
    "$\n",
    "    \\text{Output Layer:} \\\\\n",
    "$\n",
    "    $$\\begin{align}\n",
    "        Z^{[3]} &= A^{[2]} \\cdot W^{[3]} + b^{[3]} \\\\\n",
    "        A^{[3]} &= g(Z^{[3]})\n",
    "    \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining forward propagation\n",
    "def forward_prop(W1, b1, W2, b2, W3, b3, X):\n",
    "    Z1 = np.dot(X, W1) + b1  # (m, 784) . (784, 128) + (1, 128) = (m, 128)\n",
    "    A1 = ReLU(Z1)   \n",
    "    Z2 = np.dot(A1, W2) + b2  # (m, 128) . (128, 64) + (1, 64) = (m, 64)\n",
    "    A2 = ReLU(Z2)\n",
    "    Z3 = np.dot(A2, W3) + b3  # (m, 64) . (64, 10) + (1, 10) = (m, 10)\n",
    "    A3 = softmax(Z3)\n",
    "    return Z1, A1, Z2, A2, Z3, A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining derivative of activation functions\n",
    "\n",
    "$\n",
    "    \\text{Derivative of ReLU} = \\sigma '\n",
    "$\n",
    "$$\\begin{align}\n",
    "    \\sigma '(Z) = \n",
    "    \\begin{cases}\n",
    "        1 & \\text{if } Z > 0\\\\\n",
    "        0 & \\text{if } Z \\leq 0\n",
    "    \\end{cases}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining derivative functions\n",
    "def deriv_ReLU(Z):\n",
    "    return Z > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Back Propagation for Gradient descent\n",
    "$\n",
    "    \\text{Note: T denotes Transpose, m denotes the number of training examples} \\\\\n",
    "    \\text{ } \\\\\n",
    "    \\text{Output Layer:} \\\\\n",
    "$\n",
    "    $$\\begin{align}\n",
    "        dZ^{[3]} &= A^{[3]} - Y \\\\\n",
    "        dW^{[3]} &= \\frac{1}{m} \\times ( A^{[2] T} \\cdot dZ^{[3]} ) \\\\\n",
    "        db^{[3]} &= \\frac1m \\times \\sum dZ^{[3]}_i \\\\\n",
    "    \\end{align}$$\n",
    "$\n",
    "    \\text{ } \\\\\n",
    "    \\text{Hidden Layer 2:} \\\\\n",
    "$\n",
    "    $$\\begin{align}\n",
    "        dZ^{[2]} &= ( dZ^{[2]} \\cdot W^{[3] T}) \\times \\sigma ' (Z^{[2]} ) \\\\\n",
    "        dW^{[2]} &= \\frac{1}{m} \\times ( A^{[1] T} \\cdot dZ^{[2]} ) \\\\\n",
    "        db^{[2]} &= \\frac1m \\times \\sum dZ^{[2]}_i \\\\\n",
    "    \\end{align}$$\n",
    "$\n",
    "    \\text{ } \\\\\n",
    "    \\text{Hidden Layer 1:} \\\\\n",
    "$\n",
    "    $$\\begin{align}\n",
    "        dZ^{[1]} &= ( dZ^{[2]} \\cdot W^{[2] T} ) \\times \\sigma ' (Z^{[1]}) \\\\\n",
    "        dW^{[1]} &= \\frac{1}{m} \\times ( X^{T} \\cdot dZ^{[1]} ) \\\\\n",
    "        db^{[1]} &= \\frac1m \\times \\sum dZ^{[1]}_i \\\\\n",
    "    \\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining backwards propagation\n",
    "def backward_prop(Z1, Z2, A1, A2, A3, W3, W2, X, Y):\n",
    "    y_labelled = label_conversion(Y)\n",
    "    m = Y.size\n",
    "\n",
    "    dZ3 = A3 - y_labelled\n",
    "    dW3 = np.dot(A2.T, dZ3) / m\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
    "\n",
    "    dZ2 = np.dot(dZ3, W3.T) * deriv_ReLU(Z2)\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "\n",
    "    dZ1 = np.dot(dZ2, W2.T) * deriv_ReLU(Z1)\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "    \n",
    "    return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to update the parameters based on learning rate\n",
    "\n",
    "$\n",
    "    \\text{Hidden Layer 1:}\n",
    "$\n",
    "$$\\begin{align}\n",
    "    W^{[1]} &= W^{[1]} - \\alpha \\times dW^{[1]} \\\\\n",
    "    b^{[1]} &= b^{[1]} - \\alpha \\times db^{[1]}\n",
    "\\end{align}$$\n",
    "$\n",
    "    \\text{Hidden Layer 2:}\n",
    "$\n",
    "$$\\begin{align}\n",
    "    W^{[2]} &= W^{[2]} - \\alpha \\times dW^{[2]} \\\\\n",
    "    b^{[2]} &= b^{[2]} - \\alpha \\times db^{[2]}\n",
    "\\end{align}$$\n",
    "$\n",
    "    \\text{Output Layer:}\n",
    "$\n",
    "$$\\begin{align}\n",
    "    W^{[3]} &= W^{[3]} - \\alpha \\times dW^{[3]} \\\\\n",
    "    b^{[3]} &= b^{[3]} - \\alpha \\times db^{[3]}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to update the parameters\n",
    "def update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * db1\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * db2\n",
    "    W3 -= alpha * dW3\n",
    "    b3 -= alpha * db3\n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to run gradient descent\n",
    "\n",
    "Runs the actual gradient descent allowing the neural network to learn over the training examples and produce the adjusted values of weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining gradient descent\n",
    "def gradient_descent(alpha, iterations, X, Y):\n",
    "    W1, b1, W2, b2, W3, b3 = init_params()\n",
    "    for i in range(iterations + 1):\n",
    "        Z1, A1, Z2, A2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n",
    "        dW1, db1, dW2, db2, dW3, db3 = backward_prop(Z1, Z2, A1, A2, A3, W3, W2, X, Y)\n",
    "        W1, b1, W2, b2, W3, b3 = update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha)\n",
    "        # log a few results\n",
    "        if (i % 10 == 0 or i == iterations):  # Log every 10 iterations\n",
    "            cost = calculate_cost(A3, Y) \n",
    "            predictions = get_prediction(A3)\n",
    "            accuracy = get_accuracy(predictions, Y)\n",
    "            print(f'Iteration: {i} ; Cost: {cost:.2f} ; Accuracy: {accuracy:.2f}')  # Log cost and accuracy\n",
    "    return W1, b1, W2, b2, W3, b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model\n",
    "\n",
    "$\n",
    "    \\text{Setting the values of constants}\n",
    "$\n",
    "$$\\begin{align}\n",
    "    \\alpha &= 0.1 \\\\\n",
    "    \\text{Iterations} &= 1000\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 ; Cost: 2.43 ; Accuracy: 6.93\n",
      "Iteration: 10 ; Cost: 1.66 ; Accuracy: 61.96\n",
      "Iteration: 20 ; Cost: 1.08 ; Accuracy: 75.98\n",
      "Iteration: 30 ; Cost: 0.77 ; Accuracy: 81.74\n",
      "Iteration: 40 ; Cost: 0.63 ; Accuracy: 84.34\n",
      "Iteration: 50 ; Cost: 0.55 ; Accuracy: 85.97\n",
      "Iteration: 60 ; Cost: 0.49 ; Accuracy: 86.97\n",
      "Iteration: 70 ; Cost: 0.46 ; Accuracy: 87.76\n",
      "Iteration: 80 ; Cost: 0.43 ; Accuracy: 88.38\n",
      "Iteration: 90 ; Cost: 0.41 ; Accuracy: 88.78\n",
      "Iteration: 100 ; Cost: 0.39 ; Accuracy: 89.25\n",
      "Iteration: 110 ; Cost: 0.37 ; Accuracy: 89.60\n",
      "Iteration: 120 ; Cost: 0.36 ; Accuracy: 89.86\n",
      "Iteration: 130 ; Cost: 0.35 ; Accuracy: 90.12\n",
      "Iteration: 140 ; Cost: 0.34 ; Accuracy: 90.34\n",
      "Iteration: 150 ; Cost: 0.33 ; Accuracy: 90.52\n",
      "Iteration: 160 ; Cost: 0.32 ; Accuracy: 90.72\n",
      "Iteration: 170 ; Cost: 0.32 ; Accuracy: 90.88\n",
      "Iteration: 180 ; Cost: 0.31 ; Accuracy: 91.04\n",
      "Iteration: 190 ; Cost: 0.30 ; Accuracy: 91.21\n",
      "Iteration: 200 ; Cost: 0.30 ; Accuracy: 91.36\n",
      "Iteration: 210 ; Cost: 0.29 ; Accuracy: 91.51\n",
      "Iteration: 220 ; Cost: 0.29 ; Accuracy: 91.62\n",
      "Iteration: 230 ; Cost: 0.29 ; Accuracy: 91.76\n",
      "Iteration: 240 ; Cost: 0.28 ; Accuracy: 91.89\n",
      "Iteration: 250 ; Cost: 0.28 ; Accuracy: 92.02\n",
      "Iteration: 260 ; Cost: 0.27 ; Accuracy: 92.15\n",
      "Iteration: 270 ; Cost: 0.27 ; Accuracy: 92.29\n",
      "Iteration: 280 ; Cost: 0.27 ; Accuracy: 92.36\n",
      "Iteration: 290 ; Cost: 0.26 ; Accuracy: 92.46\n",
      "Iteration: 300 ; Cost: 0.26 ; Accuracy: 92.54\n",
      "Iteration: 310 ; Cost: 0.26 ; Accuracy: 92.63\n",
      "Iteration: 320 ; Cost: 0.25 ; Accuracy: 92.75\n",
      "Iteration: 330 ; Cost: 0.25 ; Accuracy: 92.82\n",
      "Iteration: 340 ; Cost: 0.25 ; Accuracy: 92.88\n",
      "Iteration: 350 ; Cost: 0.24 ; Accuracy: 92.97\n",
      "Iteration: 360 ; Cost: 0.24 ; Accuracy: 93.03\n",
      "Iteration: 370 ; Cost: 0.24 ; Accuracy: 93.11\n",
      "Iteration: 380 ; Cost: 0.24 ; Accuracy: 93.19\n",
      "Iteration: 390 ; Cost: 0.23 ; Accuracy: 93.27\n",
      "Iteration: 400 ; Cost: 0.23 ; Accuracy: 93.32\n",
      "Iteration: 410 ; Cost: 0.23 ; Accuracy: 93.41\n",
      "Iteration: 420 ; Cost: 0.23 ; Accuracy: 93.48\n",
      "Iteration: 430 ; Cost: 0.22 ; Accuracy: 93.54\n",
      "Iteration: 440 ; Cost: 0.22 ; Accuracy: 93.63\n",
      "Iteration: 450 ; Cost: 0.22 ; Accuracy: 93.71\n",
      "Iteration: 460 ; Cost: 0.22 ; Accuracy: 93.79\n",
      "Iteration: 470 ; Cost: 0.22 ; Accuracy: 93.85\n",
      "Iteration: 480 ; Cost: 0.21 ; Accuracy: 93.88\n",
      "Iteration: 490 ; Cost: 0.21 ; Accuracy: 93.93\n",
      "Iteration: 500 ; Cost: 0.21 ; Accuracy: 93.98\n",
      "Iteration: 510 ; Cost: 0.21 ; Accuracy: 94.04\n",
      "Iteration: 520 ; Cost: 0.21 ; Accuracy: 94.10\n",
      "Iteration: 530 ; Cost: 0.20 ; Accuracy: 94.14\n",
      "Iteration: 540 ; Cost: 0.20 ; Accuracy: 94.19\n",
      "Iteration: 550 ; Cost: 0.20 ; Accuracy: 94.24\n",
      "Iteration: 560 ; Cost: 0.20 ; Accuracy: 94.31\n",
      "Iteration: 570 ; Cost: 0.20 ; Accuracy: 94.37\n",
      "Iteration: 580 ; Cost: 0.20 ; Accuracy: 94.42\n",
      "Iteration: 590 ; Cost: 0.19 ; Accuracy: 94.48\n",
      "Iteration: 600 ; Cost: 0.19 ; Accuracy: 94.53\n",
      "Iteration: 610 ; Cost: 0.19 ; Accuracy: 94.56\n",
      "Iteration: 620 ; Cost: 0.19 ; Accuracy: 94.60\n",
      "Iteration: 630 ; Cost: 0.19 ; Accuracy: 94.66\n",
      "Iteration: 640 ; Cost: 0.19 ; Accuracy: 94.71\n",
      "Iteration: 650 ; Cost: 0.18 ; Accuracy: 94.76\n",
      "Iteration: 660 ; Cost: 0.18 ; Accuracy: 94.79\n",
      "Iteration: 670 ; Cost: 0.18 ; Accuracy: 94.84\n",
      "Iteration: 680 ; Cost: 0.18 ; Accuracy: 94.86\n",
      "Iteration: 690 ; Cost: 0.18 ; Accuracy: 94.92\n",
      "Iteration: 700 ; Cost: 0.18 ; Accuracy: 95.00\n",
      "Iteration: 710 ; Cost: 0.18 ; Accuracy: 95.05\n",
      "Iteration: 720 ; Cost: 0.17 ; Accuracy: 95.11\n",
      "Iteration: 730 ; Cost: 0.17 ; Accuracy: 95.15\n",
      "Iteration: 740 ; Cost: 0.17 ; Accuracy: 95.17\n",
      "Iteration: 750 ; Cost: 0.17 ; Accuracy: 95.20\n",
      "Iteration: 760 ; Cost: 0.17 ; Accuracy: 95.26\n",
      "Iteration: 770 ; Cost: 0.17 ; Accuracy: 95.30\n",
      "Iteration: 780 ; Cost: 0.17 ; Accuracy: 95.34\n",
      "Iteration: 790 ; Cost: 0.17 ; Accuracy: 95.37\n",
      "Iteration: 800 ; Cost: 0.16 ; Accuracy: 95.42\n",
      "Iteration: 810 ; Cost: 0.16 ; Accuracy: 95.47\n",
      "Iteration: 820 ; Cost: 0.16 ; Accuracy: 95.49\n",
      "Iteration: 830 ; Cost: 0.16 ; Accuracy: 95.53\n",
      "Iteration: 840 ; Cost: 0.16 ; Accuracy: 95.57\n",
      "Iteration: 850 ; Cost: 0.16 ; Accuracy: 95.61\n",
      "Iteration: 860 ; Cost: 0.16 ; Accuracy: 95.64\n",
      "Iteration: 870 ; Cost: 0.16 ; Accuracy: 95.66\n",
      "Iteration: 880 ; Cost: 0.16 ; Accuracy: 95.69\n",
      "Iteration: 890 ; Cost: 0.15 ; Accuracy: 95.71\n",
      "Iteration: 900 ; Cost: 0.15 ; Accuracy: 95.73\n",
      "Iteration: 910 ; Cost: 0.15 ; Accuracy: 95.75\n",
      "Iteration: 920 ; Cost: 0.15 ; Accuracy: 95.77\n",
      "Iteration: 930 ; Cost: 0.15 ; Accuracy: 95.80\n",
      "Iteration: 940 ; Cost: 0.15 ; Accuracy: 95.83\n",
      "Iteration: 950 ; Cost: 0.15 ; Accuracy: 95.87\n",
      "Iteration: 960 ; Cost: 0.15 ; Accuracy: 95.89\n",
      "Iteration: 970 ; Cost: 0.15 ; Accuracy: 95.93\n",
      "Iteration: 980 ; Cost: 0.15 ; Accuracy: 95.95\n",
      "Iteration: 990 ; Cost: 0.14 ; Accuracy: 95.97\n",
      "Iteration: 1000 ; Cost: 0.14 ; Accuracy: 95.99\n"
     ]
    }
   ],
   "source": [
    "# running the actual model\n",
    "W1, b1, W2, b2, W3, b3 = gradient_descent(0.1, 1000, X_train / 255.0, Y_train) # passing a normalized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to make predictions\n",
    "def make_predictions(W1, b1, W2, b2, W3, b3, X):\n",
    "    Z1, A1, Z2, A2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n",
    "    return A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions in order to test the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 94.43\n"
     ]
    }
   ],
   "source": [
    "predictions = make_predictions(W1, b1, W2, b2, W3, b3, X_test)\n",
    "accuracy = get_accuracy(get_prediction(predictions), Y_test)\n",
    "print(f'Accuracy of the model: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to save the params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to save the model parameters\n",
    "def save_params(W1, b1, W2, b2, W3, b3, filename_prefix):\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists('weights'):\n",
    "        os.makedirs('weights')\n",
    "\n",
    "    # Save the parameters to the specified directory\n",
    "    np.save(os.path.join('weights', f'{filename_prefix}_W1.npy'), W1)\n",
    "    np.save(os.path.join('weights', f'{filename_prefix}_b1.npy'), b1)\n",
    "    np.save(os.path.join('weights', f'{filename_prefix}_W2.npy'), W2)\n",
    "    np.save(os.path.join('weights', f'{filename_prefix}_b2.npy'), b2)\n",
    "    np.save(os.path.join('weights', f'{filename_prefix}_W3.npy'), W3)\n",
    "    np.save(os.path.join('weights', f'{filename_prefix}_b3.npy'), b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(W1, b1, W2, b2, W3, b3, 'model_weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to check the model's accuracy\n",
    "\n",
    "We can see how the test performs on each test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(index, W1, b1, W2, b2, W3, b3):\n",
    "    current_image = X_test[index, :, None]\n",
    "    prediction = make_predictions(W1, b1, W2, b2, W3, b3, X_test[index, :, None].T)\n",
    "    label = Y_test[index]\n",
    "    print(\"Prediction: \", np.argmax(prediction))\n",
    "    print(\"Label: \", label)\n",
    "    \n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the value of `index` to manually see how the model performs in the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  2\n",
      "Label:  2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prediction(7, W1, b1, W2, b2, W3, b3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
